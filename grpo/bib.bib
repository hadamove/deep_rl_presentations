@article{trl2015,
  author       = {John Schulman and
                  Sergey Levine and
                  Philipp Moritz and
                  Michael I. Jordan and
                  Pieter Abbeel},
  title        = {Trust Region Policy Optimization},
  journal      = {CoRR},
  volume       = {abs/1502.05477},
  year         = {2015},
  url          = {http://arxiv.org/abs/1502.05477},
  eprinttype    = {arXiv},
  eprint       = {1502.05477},
  timestamp    = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{wang2020truly,
  title={Truly proximal policy optimization},
  author={Wang, Yuhui and He, Hao and Tan, Xiaoyang},
  booktitle={Uncertainty in artificial intelligence},
  pages={113--122},
  year={2020},
  organization={PMLR}
}


@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}



@InProceedings{mirror2022,
  title = 	 {Mirror Learning: A Unifying Framework of Policy Optimisation},
  author =       {Grudzien, Jakub and De Witt, Christian A Schroeder and Foerster, Jakob},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {7825--7844},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/grudzien22a/grudzien22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/grudzien22a.html},
  abstract = 	 {Modern deep reinforcement learning (RL) algorithms are motivated by either the general policy improvement (GPI) or trust-region learning (TRL) frameworks. However, algorithms that strictly respect these theoretical frameworks have proven unscalable. Surprisingly, the only known scalable algorithms violate the GPI/TRL assumptions, e.g. due to required regularisation or other heuristics. The current explanation of their empirical success is essentially “by analogy”: they are deemed approximate adaptations of theoretically sound methods. Unfortunately, studies have shown that in practice these algorithms differ greatly from their conceptual ancestors. In contrast, in this paper, we introduce a novel theoretical framework, named Mirror Learning, which provides theoretical guarantees to a large class of algorithms, including TRPO and PPO. While the latter two exploit the flexibility of our framework, GPI and TRL fit in merely as pathologically restrictive corner cases thereof. This suggests that the empirical performance of state-of-the-art methods is a direct consequence of their theoretical properties, rather than of aforementioned approximate analogies. Mirror learning sets us free to boldly explore novel, theoretically sound RL algorithms, a thus far uncharted wonderland.}
}

@inproceedings{dpo2022,
 author = {Lu, Chris and Kuba, Jakub and Letcher, Alistair and Metz, Luke and Schroeder de Witt, Christian and Foerster, Jakob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16455--16468},
 publisher = {Curran Associates, Inc.},
 title = {Discovered Policy Optimisation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/688c7a82e31653e7c256c6c29fd3b438-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}



@misc{weng2019meta,
  author = {Lilian Weng},
  title = {Meta Reinforcement Learning},
  year = {2019},
  url = {https://lilianweng.github.io/posts/2019-06-23-meta-rl/},
  note = {Accessed: 2025-03-07}
}


@inproceedings{maheswaranathan2019guided,
  title={Guided evolutionary strategies: Augmenting random search with surrogate gradients},
  author={Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={4264--4273},
  year={2019},
  organization={PMLR}
}


@article{lu2024discovering,
  title={Discovering preference optimization algorithms with and for large language models},
  author={Lu, Chris and Holt, Samuel and Fanconi, Claudio and Chan, Alex and Foerster, Jakob and van der Schaar, Mihaela and Lange, Robert},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={86528--86573},
  year={2024}
}


@software{brax2021github,
  author = {C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
  title = {Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  url = {http://github.com/google/brax},
  version = {0.12.1},
  year = {2021},
}

@Article{young19minatar,
author = {{Young}, Kenny and {Tian}, Tian},
title = {MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments},
journal = {arXiv preprint arXiv:1903.03176},
year = "2019"
}